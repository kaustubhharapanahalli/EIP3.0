{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_2A_2B_Kaustubh_Batch_8.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "IgdEp7kXomXH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Back Propagation Step by Step\n",
        "\n",
        "The image below shows you how back propagation starts once you've performed forward propagation. \n",
        "\n",
        "![Back Propagation Image](http://hmkcode.github.io/images/ai/backpropagation.png)\n",
        "\n",
        "First we need to know why we need to write our own back propagation algorithm. Suppose if we were to build our own neural network, we'd need to perform back propagation. Without back propagation we would never get the right prediction. Let's go through this article to understand how back propagation happens.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "qS-XcQcSpfSE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this post, we will be building a neural network with three layers where 1 layer is the **input layer**, 1 layer is the **output layer** and the other layer is the **hidden layer**.\n",
        "\n",
        "We are going to consider:\n",
        "*   **Input** layer with 2 neurons or nodes\n",
        "*   **Output** layer with 1 neuron or node\n",
        "*   Finally, **Hidden** layer with 2 neurons or nodes.\n",
        "\n",
        "Here's an image which tell us what are the different parameters that are present in the neural network that we are trying to train.\n",
        "![Neural Networks image](https://i.ibb.co/D8VnkYB/nn1.png)\n",
        "\n",
        "# Weights, weights, weights\n",
        "\n",
        "Neural network training is about finding weights that minimize prediction error. We usually start our training with a set of randomly generated weights.Then, backpropagation is used to update the weights in an attempt to correctly map arbitrary inputs to outputs.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "0UuWV0k9rrW-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our initial weights will be as following: ```w1 = 0.21```, ```w2 = 0.56```, ```w3 = 0.19```, ```w4 = 0.02```, ```w5 = 0.23``` and ```w6 = 0.16```\n",
        "\n",
        "![Image with Weights](https://i.ibb.co/3SRN1YZ/bp-weights.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "vyMgkppmIPJF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "Our dataset has one sample with two inputs and one output.\n",
        "\n",
        "![Input and Actual Output Representation](https://i.ibb.co/nk0kGpW/bp-dataset.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "T3ROXYF6ISxP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our single sample is as following inputs=[8, 12] and output=[4].\n",
        "\n",
        "![Data sample](https://i.ibb.co/N9tSRVg/bp-sample.png)"
      ]
    },
    {
      "metadata": {
        "id": "m7h4nlftISmE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Forward Pass\n",
        "We will use given weights and inputs to predict the output. Inputs are multiplied by weights; the results are then passed forward to next layer.\n",
        "\n",
        "![Forward Propagation](https://i.ibb.co/QK86MGr/bp-forward.png)"
      ]
    },
    {
      "metadata": {
        "id": "hYy89MPOISBD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Calculating Error\n",
        "Now, it’s time to find out how our network performed by calculating the difference between the actual output and predicted one. It’s clear that our network output, or prediction, is not even close to actual output. We can calculate the difference or the error as following.\n",
        "\n",
        "![Error Calculation](https://i.ibb.co/yP3zZJW/bp-error.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "SFF4umWVIiBq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reducing Error\n",
        "Our main goal of the training is to reduce the error or the difference between prediction and actual output. Since actual output is constant, “not changing”, the only way to reduce the error is to change prediction value. The question now is, how to change prediction value?\n",
        "\n",
        "By decomposing prediction into its basic elements we can find that weights are the variable elements affecting prediction value. In other words, in order to change prediction value, we need to change weights values.\n",
        "\n",
        "![Reducing Error](https://i.ibb.co/t3TMGLd/bp-prediction-elements.png)"
      ]
    },
    {
      "metadata": {
        "id": "X7vynL4MIh6S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> **The question now is how to change\\update the weights value so that the error is reduced?**\n",
        "\n",
        "> **The answer is Backpropagation!**"
      ]
    },
    {
      "metadata": {
        "id": "5nUEeRobIhyS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Backpropagation\n",
        "Backpropagation, short for “backward propagation of errors”, is a mechanism used to update the weights using gradient descent. It calculates the gradient of the error function with respect to the neural network’s weights. The calculation proceeds backwards through the network.\n",
        "\n",
        ">Gradient descent is an iterative optimization algorithm for finding the minimum of a function; in our case we want to minimize th error function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient of the function at the current point.\n",
        "\n",
        "![gradient descent](https://i.ibb.co/d61ZZyt/bp-update-formula.png)"
      ]
    },
    {
      "metadata": {
        "id": "KTbdXZRDIhqC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For example, to update w6, we take the current w6 and subtract the partial derivative of error function with respect to w6. Optionally, we multiply the derivative of the error function by a selected number to make sure that the new updated weight is minimizing the error function; this number is called learning rate.\n",
        "\n",
        "![Updating weights](https://i.ibb.co/DbKtXk9/bp-w6-update.png)"
      ]
    },
    {
      "metadata": {
        "id": "o3hOhOs2I0qH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The derivation of the error function is evaluated by applying the chain rule as following:\n",
        "![Derivation of Back propagation](https://i.ibb.co/jkyPbzD/bp-error-function-partial-derivative-w6.png)"
      ]
    },
    {
      "metadata": {
        "id": "MqLF1ZTuI0js",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So to update w6 we can apply the following formula: \n",
        "\n",
        "![Updating w6 values](https://i.ibb.co/JtT2JHY/bp-w6-update-closed-form.png)"
      ]
    },
    {
      "metadata": {
        "id": "CaeBfgb8I0e4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Similarly, we can derive the update formula for w5 and any other weights existing between the output and the hidden layer.\n",
        "\n",
        "![Updating w5 values](https://i.ibb.co/GRrYmXj/bp-w5-update-closed-form.png)"
      ]
    },
    {
      "metadata": {
        "id": "S47exgBpI0ZJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "However, when moving backward to update w1, w2, w3 and w4 existing between input and hidden layer, the partial derivative for the error function with respect to w1, for example, will be as following.\n",
        "\n",
        "![Updating weight values from layer 0 to layer 1](https://i.ibb.co/09x50DP/bp-error-function-partial-derivative-w1.png)"
      ]
    },
    {
      "metadata": {
        "id": "nVNBB0CDI-qd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can find the update formula for the remaining weights w2, w3 and w4 in the same way.\n",
        "\n",
        "In summary, the update formulas for all weights will be as following:\n",
        "\n",
        "![Updating all weights](https://i.ibb.co/HnxhszX/bp-update-all-weights.png)"
      ]
    },
    {
      "metadata": {
        "id": "-YylIWnmCll3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can rewrite the update formulas in matrices as following:\n",
        "\n",
        "![Updating all wieights in matrix](https://i.ibb.co/25b1fwb/bp-update-all-weights-matrix.png)"
      ]
    },
    {
      "metadata": {
        "id": "Qj6ST2LoI-fO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Backward Pass\n",
        "Using derived formulas we can find the new weights.\n",
        "\n",
        "Learning rate: is a hyperparameter which means that we need to manually guess its value.\n",
        "\n",
        "![Update all weights values](https://i.ibb.co/njz0djw/bp-new-weights.png)"
      ]
    },
    {
      "metadata": {
        "id": "MMgXueKdJD62",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, using the new weights we will repeat the forward passed.\n",
        "\n",
        "![New forward propagation](https://i.ibb.co/685qtN7/bp-forward.png)"
      ]
    },
    {
      "metadata": {
        "id": "Hus_c0xjJFvf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can notice that the prediction 13.95 is very far from the actual output than the previously predicted value 2.2136. We have to repeat the process of backward pass and forward pass until error is close or equal to zero."
      ]
    },
    {
      "metadata": {
        "id": "15HRotDAUiva",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "t7P6CT6bU5Mu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Code for Neural Networks with Back Propagation"
      ]
    },
    {
      "metadata": {
        "id": "vtB2QS00I90c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Hq4yrLjVA1V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_data = np.array([[8], [12]])\n",
        "output_data = np.array([4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XylDzb3VwB6j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w0 = np.array([[0.21, 0.19],[0.56, 0.02]])\n",
        "w1 = np.array([[0.23], [0.16]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SBdppuPUvWLw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward_pass(data_in, w0,w1):\n",
        "    layer0 = data_in.T\n",
        "    layer1 = np.dot(layer0, w0)\n",
        "    layer2 = np.dot(layer1, w1)\n",
        "    \n",
        "#     print(w0, w1)\n",
        "\n",
        "    return layer0, layer1, layer2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "znk1d9ZcvW8v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def backpropogate(i, layer0, layer1, layer2, actual_y, w0,w1, learning_rate):\n",
        "    delta = layer2 - output_data\n",
        "    \n",
        "    w1_1 = w1.copy()\n",
        "    w1 = w1 - (learning_rate * delta * layer1).reshape(2,1)   \n",
        "    w0 = w0 - (learning_rate * delta * w1_1.T * input_data)\n",
        "\n",
        "    if i%1==0:\n",
        "        loss = np.mean(np.power(layer2-actual_y, 2))*0.5\n",
        "        print(\"\\n\", int(i), loss)\n",
        "        \n",
        "    return w0, w1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Nn1N4orFJBO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FO_Y-cQHEoTg",
        "colab_type": "code",
        "outputId": "784a6a6b-cbef-498f-9a79-a3aa5b4b19e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(epochs):\n",
        "  layer0, layer1, layer2 = forward_pass(input_data, w0,w1)\n",
        "  w0,w1 = backpropogate(i,layer0, layer1, layer2, output_data, w0,w1, 0.05 )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 0 1.59561248\n",
            "\n",
            " 1 49.24728928473661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b8nEfMJ9FObg",
        "colab_type": "code",
        "outputId": "39555df4-8ee1-4d91-f173-b323bf16a904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "  layer2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[13.92444349]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "r_3aqV-8KeCT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}